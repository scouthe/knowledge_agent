import os
import json
import time
import hashlib
import chromadb
from chromadb.utils import embedding_functions
from config import (
    OBSIDIAN_ROOT, CHROMA_DB_PATH, EMBEDDING_API_URL, EMBEDDING_MODEL_NAME,
    CHROMA_COLLECTION_NAME, MIN_CONTENT_LENGTH, CHUNK_SIZE, CHUNK_OVERLAP
)
from utils.helpers import sanitize_filename, url_hash

# === 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ ===
print("ğŸ§  æ­£åœ¨åˆå§‹åŒ– ChromaDB...")
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

# è‡ªå®šä¹‰ OpenAI å…¼å®¹çš„ Embedding å‡½æ•°
emb_fn = embedding_functions.OpenAIEmbeddingFunction(
    api_key="lm-studio",
    api_base=EMBEDDING_API_URL,
    model_name=EMBEDDING_MODEL_NAME
)

collection = chroma_client.get_or_create_collection(
    name=CHROMA_COLLECTION_NAME,
    embedding_function=emb_fn
)
print(f"âœ… ChromaDB å°±ç»ª: {CHROMA_COLLECTION_NAME}")


# === 2. å·¥å…·å‡½æ•°ï¼šæ–‡æœ¬åˆ†å— (Simple Chunking) ===
def split_text_into_chunks(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """
    ç®€å•çš„æ–‡æœ¬åˆ†å—ç­–ç•¥ï¼š
    1. å…ˆæŒ‰åŒæ¢è¡Œç¬¦ \n\n åˆ‡åˆ† (æ®µè½)
    2. å¦‚æœæ®µè½å¤ªé•¿ï¼Œå†å¼ºè¡Œæˆªæ–­
    """
    if not text: return []
    
    chunks = []
    # æŒ‰æ®µè½ç²—åˆ†
    paragraphs = text.split('\n\n')
    
    current_chunk = ""
    
    for para in paragraphs:
        para = para.strip()
        if not para: continue
        
        # å¦‚æœå½“å‰å— + æ–°æ®µè½ æ²¡è¶…é™ï¼Œå°±æ‹¼èµ·æ¥
        if len(current_chunk) + len(para) < chunk_size:
            current_chunk += "\n\n" + para
        else:
            # å¦‚æœè¶…é™äº†ï¼Œå…ˆæŠŠæ—§çš„å­˜äº†
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # å¦‚æœè¿™ä¸€æ®µæœ¬èº«å°±å·¨é•¿ (è¶…è¿‡ chunk_size)ï¼Œåªèƒ½å¼ºè¡Œåˆ‡åˆ†
            if len(para) > chunk_size:
                for i in range(0, len(para), chunk_size - overlap):
                    chunks.append(para[i:i + chunk_size])
                current_chunk = "" # åˆ‡å®Œæ¸…ç©º
            else:
                # è¿™ä¸€æ®µä½œä¸ºæ–°å—çš„å¼€å§‹
                current_chunk = para
                
    # æœ€åä¸€ä¸ªæ²¡å­˜çš„å­˜è¿›å»
    if current_chunk:
        chunks.append(current_chunk.strip())
        
    return chunks


# === 3. æ ¸å¿ƒï¼šä¿å­˜åˆ° Markdown (Truth) ===
# (è¿™éƒ¨åˆ†å’Œä½ æ˜¨å¤©çš„ä»£ç åŸºæœ¬ä¸€è‡´ï¼Œä¿ç•™å³å¯)
def format_analysis_to_markdown(analysis_data):
    if not analysis_data: return "> æš‚æ— åˆ†æ"
    lines = []
    if isinstance(analysis_data, dict):
        for k, v in analysis_data.items(): lines.append(f"* ğŸ“Œ **{k}**: {v}")
    elif isinstance(analysis_data, str):
        lines = analysis_data.split('\n')
    else:
        lines = [str(analysis_data)]
    return "\n".join([f"> {line}" for line in lines if line.strip()])

def save_to_obsidian(raw_data: dict, ai_data: dict):
    # ... (æ­¤å¤„ä¿ç•™æ˜¨å¤© save_to_obsidian çš„å®Œæ•´ä»£ç ï¼Œæ— éœ€æ”¹åŠ¨) ...
    # ä»…ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œç•¥è¿‡ï¼Œè¯·ç›´æ¥å¤åˆ¶æ˜¨å¤©çš„é€»è¾‘
    # è®°å¾—æœ€å return full_path, doc_id  <-- ç¨å¾®æ”¹ä¸€ä¸‹è¿”å›å€¼ï¼Œæ–¹ä¾¿ pipeline ç”¨
    
    category = raw_data.get("category", "æ–‡ç« é˜…è¯»")
    url = raw_data.get("url", "")
    content = raw_data.get("content", "")
    
    doc_id = raw_data.get("doc_id") or (url_hash(url) if url else hashlib.md5(content.encode()).hexdigest())
    hash6 = doc_id[:6]
    
    title = ai_data.get("kb_title", raw_data.get("title", "æ— æ ‡é¢˜"))
    safe_title = sanitize_filename(title)
    
    now = time.localtime()
    year_month = time.strftime("%Y-%m", now)
    date_short = time.strftime("%m-%d", now)
    note_ts = time.strftime("%m-%d_%H%M", now)
    
    if category == "ä¸ªäººç¬”è®°":
        folder = "Notes"
        file_name = f"{note_ts}_{safe_title}__{hash6}.md"
    else:
        folder = "Articles"
        source = "å…¶ä»–"
        if "zhihu" in url: source = "çŸ¥ä¹"
        elif "xiaohongshu" in url: source = "å°çº¢ä¹¦"
        elif "weixin" in url: source = "å…¬ä¼—å·"
        elif raw_data.get("site"): source = raw_data.get("site")
        file_name = f"{source}-{date_short}-{safe_title}__{hash6}.md"

    dir_path = os.path.join(OBSIDIAN_ROOT, folder, year_month)
    os.makedirs(dir_path, exist_ok=True)
    full_path = os.path.join(dir_path, file_name)
    
    formatted_analysis = format_analysis_to_markdown(ai_data.get("analysis"))
    meta = {
        "created": time.strftime("%Y-%m-%d %H:%M", now),
        "source": url,
        "category": category,
        "tags": ai_data.get("tags", []),
        "kb_title": safe_title,
        "doc_id": doc_id,
        "url_hash": doc_id if url else ""
    }
    
    frontmatter = "\n".join([f"{k}: {json.dumps(v, ensure_ascii=False) if isinstance(v, list) else v}" for k, v in meta.items()])
    callout = "ğŸ’¡ ç¬”è®°æ•´ç†" if category == "ä¸ªäººç¬”è®°" else "ğŸ“– AI æ·±åº¦å¯¼è¯»"
    
    md = f"---\n{frontmatter}\n---\n\n# {safe_title}\n\n> [!ABSTRACT] {callout}\n{formatted_analysis}\n\n---\n\n## åŸæ–‡å†…å®¹\n\n{content}\n"
    
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(md)
    
    print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {full_path}")
    return full_path, doc_id  # <--- æ³¨æ„ï¼šå¤šè¿”å›äº†ä¸€ä¸ª doc_id


# === 4. æ ¸å¿ƒï¼šä¿å­˜åˆ°å‘é‡åº“ (Brain) ===
def save_to_vector_db(raw_data: dict, ai_data: dict, file_path: str, doc_id: str):
    """
    åˆ†å—å­˜å…¥å‘é‡åº“ï¼Œæ”¯æŒå¹‚ç­‰æ›´æ–°ï¼ˆå…ˆåˆ åå†™ï¼‰
    """
    content = raw_data.get("content", "")
    if len(content) < MIN_CONTENT_LENGTH:
        print("âš ï¸ å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡å‘é‡åŒ–")
        return 0 # è¿”å›æ’å…¥æ•°é‡

    title = ai_data.get("kb_title", "æ— æ ‡é¢˜")
    category = raw_data.get("category", "æ–‡ç« é˜…è¯»")
    url = raw_data.get("url", "")
    
    # 1. å¹‚ç­‰æ¸…ç†ï¼šå…ˆåˆ é™¤æ—§çš„ (åŸºäº metadata parent_id)
    # è¿™æ ·å¦‚æœæ–‡ç« æ›´æ–°äº†ï¼Œæ—§çš„åˆ‡ç‰‡ä¼šè¢«æ¸…é™¤ï¼Œä¸ä¼šæœ‰æ®‹ç•™
    try:
        collection.delete(where={"parent_id": doc_id})
    except Exception:
        pass # å¦‚æœä¸å­˜åœ¨ä¹Ÿæ²¡å…³ç³»

    # 2. æ–‡æœ¬åˆ†å—
    chunks = split_text_into_chunks(content)
    if not chunks:
        return 0

    # 3. æ„é€ å‘é‡æ•°æ®
    ids = []
    documents = []
    metadatas = []
    
    created_at = time.strftime("%Y-%m-%d %H:%M:%S")

    for i, chunk_text in enumerate(chunks):
        # å”¯ä¸€ ID: æ–‡æ¡£ID_å—åºå·
        chunk_id = f"{doc_id}_{i}"
        
        meta = {
            "parent_id": doc_id,    # å…³é”®ï¼šç”¨äºå…³è”æ•´ç¯‡æ–‡ç« 
            "chunk_idx": i,
            "title": title,
            "category": category,
            "source": url,
            "file_path": file_path,
            "created_at": created_at
        }
        
        ids.append(chunk_id)
        documents.append(chunk_text)
        metadatas.append(meta)

    # 4. æ‰¹é‡å†™å…¥ Chroma
    # è¿™é‡Œçš„ documents ä¼šè¢«è‡ªåŠ¨ Embedding
    collection.upsert(
        ids=ids,
        documents=documents,
        metadatas=metadatas
    )
    
    print(f"ğŸ§  å‘é‡åŒ–å®Œæˆ: {title} -> åˆ‡åˆ† {len(chunks)} å—")
    return len(chunks)