import re
import time
import os
import hashlib
from utils.logger import append_job_event
from utils.helpers import url_hash
from core.crawler import fetch_via_trafilatura, fetch_via_jina
from core.llm import call_llm_analysis
from core.storage import save_to_obsidian
from core.wechat import send_wecom_msg
from core.index import save_to_keyword_index # å¼•å…¥æ–°æ¨¡å—
from core.storage import save_to_obsidian, save_to_vector_db # å¼•å…¥æ–°å‡½æ•°

async def process_content_to_obsidian(job_id: str, content: str, user_id: str):
    t0 = time.time()
    append_job_event(job_id, "RUNNING", step="start", user_id=user_id)
    
    url_pattern = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[^\s]*'
    urls = re.findall(url_pattern, content)
    payload = {}
    target_url = ""

    # === 1. æŠ“å–é˜¶æ®µ ===
    if urls:
        target_url = urls[0]
        use_jina = "xiaohongshu.com" in target_url or "xhslink.com" in target_url
        
        if not use_jina:
            append_job_event(job_id, "RUNNING", step="crawl_local", url=target_url)
            res = await fetch_via_trafilatura(target_url)
            if res: payload = res
            else: use_jina = True
            
        if use_jina:
            append_job_event(job_id, "RUNNING", step="crawl_jina", url=target_url)
            res = await fetch_via_jina(target_url)
            if res: 
                payload = res
                payload["category"] = "æ–‡ç« é˜…è¯»"
            else:
                payload = {"type": "error", "msg": "æŠ“å–å¤±è´¥", "url": target_url}
        
        if payload.get("type") != "error":
            payload["doc_id"] = url_hash(target_url)
            
    else:
        # ä¸ªäººç¬”è®°
        payload = {
            "type": "note",
            "category": "ä¸ªäººç¬”è®°",
            "content": content,
            "title": f"éšæ‰‹è®°_{content[:10].replace(chr(10), ' ')}",
            "doc_id": hashlib.md5(content.encode()).hexdigest()
        }

    # é”™è¯¯ç†”æ–­
    if payload.get("type") == "error":
        append_job_event(job_id, "FAILED", step="crawl", error=payload.get("msg"))
        await send_wecom_msg(user_id, f"âŒ å…¥åº“å¤±è´¥: {payload.get('msg')}")
        return

    # === 2. AI åˆ†æ ===
    try:
        ai_res = await call_llm_analysis(payload["content"], payload["category"])
    except Exception as e:
        await send_wecom_msg(user_id, f"âš ï¸ AI å¤±è´¥: {e}")
        return

    # === 3. ä¿å­˜ (åŒå†™æ¨¡å¼) ===
    try:
        # A. å­˜æ–‡ä»¶ (Truth)
        # æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶ä¸¤ä¸ªè¿”å›å€¼
        path, doc_id = save_to_obsidian(payload, ai_res)
        
        # B. å­˜å‘é‡ (Brain)
        append_job_event(job_id, "RUNNING", step="save_vector_start", message="å¼€å§‹å‘é‡åŒ–...")
        
        chunk_count = save_to_vector_db(payload, ai_res, path, doc_id)
        
        append_job_event(job_id, "RUNNING", step="save_vector_success", 
                         message=f"å‘é‡åŒ–å®Œæˆï¼Œåˆ‡åˆ† {chunk_count} å—",
                         extra={"chunk_count": chunk_count, "doc_id": doc_id})
        
    except Exception as e:
        await send_wecom_msg(user_id, f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")
        append_job_event(job_id, "FAILED", step="save_error", error=str(e))
        print(f"âŒ ä¿å­˜æµç¨‹å¼‚å¸¸: {e}")
        return

    # === 4. é€šçŸ¥ ===
    try:
        # 1. å…ˆå®šä¹‰å˜é‡
        file_name = os.path.basename(path)
        duration = round(time.time() - t0, 2)
        
        # 2. å‘é€å¾®ä¿¡
        ok = await send_wecom_msg(user_id, f"âœ… **å…¥åº“æˆåŠŸ**\nğŸ“„ {file_name}")
        status = "SUCCESS" if ok else "SUCCESS_NOTIFY_FAIL"
        
        # 3. è®°æ—¥å¿—
        append_job_event(job_id, status, step="done", message=f"è€—æ—¶ {duration}s")
        
        # 4. æ§åˆ¶å°æ‰“å° (ç°åœ¨å˜é‡éƒ½æœ‰äº†)
        print(f"âœ… ä»»åŠ¡ç»“æŸ [è€—æ—¶ {duration}s]: {file_name}")
    except Exception:
        pass