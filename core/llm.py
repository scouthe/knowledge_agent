import json
import httpx
import time
from config import LLM_API_URL, LLM_MODEL

async def call_llm_analysis(content: str, category: str):
    print(f"ğŸ§  AI åˆ†æä¸­... [{category}]")
    
    if category == "ä¸ªäººç¬”è®°":
        instruction = "å¯¹ç¬”è®°è¿›è¡Œæ¶¦è‰²ï¼Œè¿”å› kb_title, summary, tags, analysis(æ ¸å¿ƒæƒ³æ³•/è¡ŒåŠ¨å»ºè®®/å…³è”æ¦‚å¿µ)ã€‚"
    else:
        instruction = "å¯¹æ–‡ç« è¿›è¡Œæ·±åº¦å¯¼è¯»ï¼Œè¿”å› kb_title, summary, tags, analysis(èƒŒæ™¯/è§‚ç‚¹/ç»“è®º)ã€‚"

    system_prompt = f"ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çŸ¥è¯†åº“ç®¡ç†å‘˜ã€‚è¯·æ ¹æ®å†…å®¹ç±»å‹ï¼š{category}ï¼Œä¸¥æ ¼ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚\n{instruction}\nä¸è¦åŒ…å«Markdownæ ‡è®°ã€‚"

    payload = {
        "model": LLM_MODEL,
        "temperature": 0.1,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"å†…å®¹ï¼š\n{content[:25000]}"}
        ]
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(LLM_API_URL, json=payload)
            resp.raise_for_status()
            raw = resp.json()['choices'][0]['message']['content']
            clean = raw.replace("```json", "").replace("```", "").strip()
            return json.loads(clean)
    except Exception as e:
        print(f"âŒ LLM å¤±è´¥: {e}")
        return {
            "kb_title": f"æœªå‘½å_{int(time.time())}",
            "summary": "AI åˆ†æå¤±è´¥",
            "tags": ["AI_Error"],
            "analysis": f"é”™è¯¯: {str(e)}"
        }
    
def chat(user_query: str, system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚") -> str:
    """
    é€šç”¨å¯¹è¯å‡½æ•°ï¼Œä¾› Web UI (RAG) ä½¿ç”¨
    """
    print(f"ğŸ¤– LLM æ­£åœ¨æ€è€ƒ: {user_query[:20]}...")
    
    headers = {
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": LLM_MODEL, # ç¡®ä¿ä½ çš„ config é‡Œæœ‰ MODEL_NAMEï¼Œæ²¡æœ‰çš„è¯å†™æ­»å­—ç¬¦ä¸²ä¹Ÿå¯ä»¥
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ],
        "temperature": 0.7,
        "max_tokens": 2000,
        "stream": False
    }

    try:
        # ä½¿ç”¨ httpx å‘é€è¯·æ±‚ (å¤ç”¨ä¹‹å‰çš„ LLM_API_URL)
        response = httpx.post(
            f"{LLM_API_URL}/v1/chat/completions", 
            headers=headers, 
            json=payload, 
            timeout=60.0 # RAG æ£€ç´¢é˜…è¯»é‡å¤§ï¼Œè¶…æ—¶è®¾é•¿ä¸€ç‚¹
        )
        response.raise_for_status()
        
        # è§£æè¿”å›ç»“æœ
        result = response.json()
        answer = result['choices'][0]['message']['content']
        return answer

    except Exception as e:
        print(f"âŒ Chat æ¥å£è°ƒç”¨å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œæˆ‘çš„å¤§è„‘ï¼ˆLLMï¼‰æš‚æ—¶è¿æ¥ä¸ä¸Šã€‚é”™è¯¯ä¿¡æ¯ï¼š{e}"