import json
import httpx
import time
from config import LLM_API_URL, LLM_MODEL

async def call_llm_analysis(content: str, category: str):
    print(f"ğŸ§  AI åˆ†æä¸­... [{category}]")
    
    if category == "ä¸ªäººç¬”è®°":
        instruction = "å¯¹ç¬”è®°è¿›è¡Œæ¶¦è‰²ï¼Œè¿”å› kb_title, summary, tags, analysis(æ ¸å¿ƒæƒ³æ³•/è¡ŒåŠ¨å»ºè®®/å…³è”æ¦‚å¿µ)ã€‚"
    else:
        instruction = "å¯¹æ–‡ç« è¿›è¡Œæ·±åº¦å¯¼è¯»ï¼Œè¿”å› kb_title, summary, tags, analysis(èƒŒæ™¯/è§‚ç‚¹/ç»“è®º)ã€‚"

    system_prompt = f"ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çŸ¥è¯†åº“ç®¡ç†å‘˜ã€‚è¯·æ ¹æ®å†…å®¹ç±»å‹ï¼š{category}ï¼Œä¸¥æ ¼ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚\n{instruction}\nä¸è¦åŒ…å«Markdownæ ‡è®°ã€‚"

    payload = {
        "model": LLM_MODEL,
        "temperature": 0.1,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"å†…å®¹ï¼š\n{content[:15000]}"}
        ]
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(LLM_API_URL, json=payload)
            resp.raise_for_status()
            raw = resp.json()['choices'][0]['message']['content']
            clean = raw.replace("```json", "").replace("```", "").strip()
            return json.loads(clean)
    except Exception as e:
        print(f"âŒ LLM å¤±è´¥: {e}")
        return {
            "kb_title": f"æœªå‘½å_{int(time.time())}",
            "summary": "AI åˆ†æå¤±è´¥",
            "tags": ["AI_Error"],
            "analysis": f"é”™è¯¯: {str(e)}"
        }